{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into RedShift\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)`\n",
    "\n",
    "After running this notebook, save this notebook and push the changes to CodeCommit. This will cause Continous Integration (CI) Pipeline to run and deploy the staging endpoint. \n",
    "\n",
    "Instructions to perform the git related operations are in the [**instructions.md**](instructions.md) and also shown at the end of this notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of AWS services used in this notebook\n",
    "\n",
    "AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides all the capabilities needed for data integration so that you can start analyzing your data and putting it to use in minutes instead of months.\n",
    "\n",
    "Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena is easy to use. Simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds. With Athena, thereâ€™s no need for complex ETL jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.\n",
    "\n",
    "Athena is out-of-the-box integrated with AWS Glue Data Catalog, allowing you to create a unified metadata repository across various services, crawl data sources to discover schemas and populate your Catalog with new and modified table and partition definitions, and maintain schema versioning.\n",
    "\n",
    "Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to execute very fast against large datasets. Much of the processing occurs in the Redshift Spectrum layer, and most of the data remains in Amazon S3. Multiple clusters can concurrently query the same dataset in Amazon S3 without the need to make copies of the data for each cluster.\n",
    "\n",
    "## Methods to load data into RedShift\n",
    "There are multiple methods to load data into RedShift:\n",
    "- Copy command\n",
    "- Insert command\n",
    "- Via other AWS services like Glue \n",
    "\n",
    "This notebook uses the insert command where the data is stored in S3 and loaded into RedShift using Athena. The main purpose is to illustrate RedShift spectrum. \n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will create the Glue database and tables, referencing the CSV file that will be uploaded to S3. Thereafter, you will use RedShift Spectrum to query data from Glue while the data continues to reside in S3. At the end, you will insert data into RedShift using Athena. \n",
    "\n",
    "Note: The codes in this notebook uses the JDBC way to access RedShift while the Python code in the lambda function (lambda_redshift_dl.py) uses the new Amazon RedShift Data API to access RedShift. Both methods accomplish the task.\n",
    "\n",
    "From the [blog post](https://aws.amazon.com/blogs/big-data/using-the-amazon-redshift-data-api-to-interact-with-amazon-redshift-clusters/) describing RedShift Data API:\n",
    "\n",
    "As a data engineer or application developer, for some use cases, you want to interact with Amazon Redshift to load or query data with a simple API endpoint without having to manage persistent connections. With Amazon Redshift Data API, you can interact with Amazon Redshift without having to configure JDBC or ODBC. This makes it easier and more secure to work with Amazon Redshift and opens up new use cases.\n",
    "\n",
    "---\n",
    "\n",
    "## Data flow in this notebook (simplified)\n",
    "\n",
    "![data](img/data.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "Variable name for secret in Secret Manager. RedShift, Athena and Glue information are stored in the secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_name='bankdm_redshift_login' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import libraries\n",
    "pyathena is used to connect to Athena while sqlalchemy is used to connect to RedShift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q SQLAlchemy==1.3.13\n",
    "!pip install psycopg2-binary pyathena\n",
    "!pip install -U pip\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from pyathena import connect\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create client sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get region \n",
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "# Get SageMaker session & default S3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "redshift = boto3.client('redshift')\n",
    "secretsmanager = boto3.client('secretsmanager')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get credentials & connection information from Secret Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    get_secret_value_response = secretsmanager.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    secret_arn=get_secret_value_response['ARN']\n",
    "\n",
    "except ClientError as e:\n",
    "    print(\"Error retrieving secret. Error: \" + e.response['Error']['Message'])\n",
    "    \n",
    "else:\n",
    "    # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "    if 'SecretString' in get_secret_value_response:\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    else:\n",
    "        secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            \n",
    "secret_json = json.loads(secret)\n",
    "master_user_name = secret_json['username']\n",
    "master_user_pw = secret_json['password']\n",
    "redshift_port = secret_json['port']\n",
    "redshift_cluster_identifier = secret_json['dbClusterIdentifier']\n",
    "redshift_endpoint_address = secret_json['host']\n",
    "\n",
    "database_name_redshift = secret_json['database_name_redshift']\n",
    "database_name_glue = secret_json['database_name_glue']\n",
    "\n",
    "schema_redshift = secret_json['schema_redshift']\n",
    "schema_athena = secret_json['schema_athena']\n",
    "\n",
    "table_name_glue = secret_json['table_name_glue']\n",
    "table_name_redshift = secret_json['table_name_redshift']\n",
    "\n",
    "# print(master_user_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy data (bank-additional.csv) to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('bank-additional/bank-additional-full.csv', bucket, 'bankdm/data/bank-additional.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glue & Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Glue database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set S3 staging directory -- this is a temporary directory used for Athena queries\n",
    "s3_staging_dir = \"s3://{0}/athena/staging\".format(bucket)\n",
    "conn = connect(region_name=region_name, s3_staging_dir=s3_staging_dir)\n",
    "statement = \"CREATE DATABASE IF NOT EXISTS {}\".format(database_name_glue)\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the database is created successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_sql(statement, conn)\n",
    "statement = \"SHOW DATABASES\"\n",
    "\n",
    "df_show = pd.read_sql(statement, conn)\n",
    "df_show.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Glue table referencing the data in the CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'default' column name causes an error and the name is changed to 'defaulted' instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bankdm_path = \"s3://{}/bankdm/data/\".format(bucket)\n",
    "# SQL statement to execute\n",
    "statement = \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
    "         age int,\n",
    "         job string,\n",
    "         marital string,\n",
    "         education string,\n",
    "         defaulted string,\n",
    "         housing string,\n",
    "         loan string,\n",
    "         contact string,\n",
    "         month string,\n",
    "         day_of_week string,\n",
    "         duration int,\n",
    "         campaign int,\n",
    "         pdays int,\n",
    "         previous int,\n",
    "         poutcome string,\n",
    "         emp_var_rate float,\n",
    "         cons_price_idx float,\n",
    "         cons_conf_idx float,\n",
    "         euribor3m float,\n",
    "         nr_employed int,\n",
    "         y string\n",
    ") \n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ',' \n",
    "LINES TERMINATED BY '\\n' \n",
    "LOCATION '{}'\n",
    "TBLPROPERTIES ('compressionType'='gzip', 'skip.header.line.count'='1')\"\"\".format(database_name_glue, table_name_glue, s3_bankdm_path)\n",
    "\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the table is created successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_sql(statement, conn)\n",
    "statement = \"SHOW TABLES in {}\".format(database_name_glue)\n",
    "\n",
    "df_show = pd.read_sql(statement, conn)\n",
    "df_show.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test getting data from the table using Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"SELECT * FROM {}.{}\n",
    "\"\"\".format(\n",
    "    database_name_glue, table_name_glue\n",
    ")\n",
    "\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(statement, conn)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RedShift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to RedShift\n",
    "#### Before connecting, ensure that the cluster is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "response = redshift.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "print(cluster_status)\n",
    "\n",
    "while cluster_status != 'available':\n",
    "    time.sleep(10)\n",
    "    response = redshift.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "    cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "    print(cluster_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also ensure the `ApplyStatus` is `in-sync`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_role = response['Clusters'][0]['IamRoles'][0]['IamRoleArn']\n",
    "\n",
    "response['Clusters'][0]['IamRoles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check the RedShift endpoint and IAM role by uncommenting the code below\n",
    "# print('Redshift endpoint: {}'.format(redshift_endpoint_address))\n",
    "# print('IAM Role: {}'.format(iam_role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the checks are done, connect to RedShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(master_user_name, master_user_pw, redshift_endpoint_address, redshift_port, database_name_redshift))\n",
    "session = sessionmaker()\n",
    "session.configure(bind=engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RedShift schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"CREATE SCHEMA IF NOT EXISTS {}\"\"\".format(schema_redshift)\n",
    "\n",
    "s = session()\n",
    "s.execute(statement)\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Glue database with Redshift Spectrum to access the data directly in S3\n",
    "\n",
    "With just one command, you can query the S3 data lake from Amazon RedShift without moving any data into the data warehouse. This is the power of Redshift Spectrum. \n",
    "\n",
    "Note the `FROM DATA CATALOG` below.  This is pulling the table and schema information from the Glue Data Catalog (ie. Hive Metastore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "CREATE EXTERNAL SCHEMA IF NOT EXISTS {} FROM DATA CATALOG \n",
    "    DATABASE '{}' \n",
    "    IAM_ROLE '{}'\n",
    "    REGION '{}'\n",
    "    CREATE EXTERNAL DATABASE IF NOT EXISTS\n",
    "\"\"\".format(schema_athena, database_name_glue, iam_role, region_name)\n",
    "\n",
    "print(statement)\n",
    "s = session()\n",
    "s.execute(statement)\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Sample Query on S3 Data through Redshift Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT *\n",
    "    FROM {}.{}\n",
    "\"\"\".format(schema_athena, table_name_glue)\n",
    "\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(statement, engine)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table in RedShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {}.{}( \n",
    "     age integer,\n",
    "     job text,\n",
    "     marital text,\n",
    "     education text,\n",
    "     defaulted text,\n",
    "     housing text,\n",
    "     loan text,\n",
    "     contact text,\n",
    "     month text,\n",
    "     day_of_week text,\n",
    "     duration integer,\n",
    "     campaign integer,\n",
    "     pdays integer,\n",
    "     previous integer,\n",
    "     poutcome text,\n",
    "     emp_var_rate decimal,\n",
    "     cons_price_idx decimal,\n",
    "     cons_conf_idx decimal,\n",
    "     euribor3m decimal,\n",
    "     nr_employed integer,\n",
    "     y text\n",
    "     )\n",
    "\"\"\".format(schema_redshift, table_name_redshift)\n",
    "\n",
    "print(statement)\n",
    "s = session()\n",
    "s.execute(statement)\n",
    "s.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data from S3 to RedShift using Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "INSERT INTO {}.{}\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        {}.{};             \n",
    "\n",
    "\"\"\".format(schema_redshift, table_name_redshift, schema_athena, table_name_glue)\n",
    "print(statement)\n",
    "s = session()\n",
    "s.execute(statement)\n",
    "s.commit()        \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test getting data from RedShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT *\n",
    "    FROM {}.{}\n",
    "\"\"\".format(schema_redshift, table_name_redshift)\n",
    "\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(statement, engine)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next steps\n",
    "\n",
    "In this notebook, you have created the Glue table referring the data from the CSV file stored in S3. After that, you inserted data into RedShift using a SQL statement to select all the rows. This is one method of loading data into RedShift.\n",
    "\n",
    "> ðŸ”¥Note: After running this notebook, you can save this notebook, commit and push the changes to CodeCommit. This will force the SageMaker Pipeline to run after a short while. The SageMaker Pipeline must run successfully before you proceed with the next notebook! This takes around 12 minutes. ðŸ”¥\n",
    "\n",
    "Instructions to perform the git related operations are shown below. These are taken from **instructions.md**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On the left side, click on the second icon. Scroll your mouse to the right of `Changed`, select the `+` to track all files. Repeat the same for `Untracked`.\n",
    "\n",
    "![studio](img/studio16.png)\n",
    "\n",
    "- Your window should look like this where there are no files under `Changed` and `Untracked`. The number of files shown may differ from yours.\n",
    "- Enter a commit message (commit in the screenshot) and click `Commit`.\n",
    "\n",
    "![studio](img/studio17.png)\n",
    "\n",
    "- Enter your name, email and click `OK`.\n",
    "\n",
    "![studio](img/studio18.png)\n",
    "\n",
    "- Click on the icon with an up arrow to push the changes. This icon is above the green line in the screenshot.\n",
    "\n",
    "![studio](img/studio19.png)\n",
    "\n",
    "- You will get the following message.\n",
    "\n",
    "![studio](img/studio20.png)\n",
    "\n",
    "- On the right side of the screen, click on `Pipelines` tab and double click on the Pipelines shown (`BankDM-p-7cj6qm9kexri` in the screenshot).\n",
    "\n",
    "![studio](img/studio21.png)\n",
    "\n",
    "- The pipeline should automatically run after a short while as you pushed in new codes.\n",
    "\n",
    "![studio](img/studio22-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
