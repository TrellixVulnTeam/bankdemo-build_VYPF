import argparse
import logging
import pathlib

import boto3
import numpy as np
import pandas as pd

logger = logging.getLogger()
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler())

if __name__ == "__main__":
    logger.info("Starting preprocessing.")
    parser = argparse.ArgumentParser()
    parser.add_argument("--input-data", type=str, required=True)
    args = parser.parse_args()

    base_dir = "/opt/ml/processing"
    pathlib.Path(f"{base_dir}/data").mkdir(parents=True, exist_ok=True)
    input_data = args.input_data
    logger.info("Input data: %s", input_data)
    bucket = input_data.split("/")[2]
    key = "/".join(input_data.split("/")[3:])

    logger.info("Downloading data from bucket: %s, key: %s", bucket, key)
    fn = f"{base_dir}/data/raw-data.csv"
    s3 = boto3.resource("s3")
    s3.Bucket(bucket).download_file(key, fn)

    #fn = 'raw-data.csv'

    logger.info("Reading downloaded data.")

    # read in csv
    df = pd.read_csv(fn)

    df.to_csv(f"{base_dir}/raw/raw.csv",index=False,header=True)
    
#     rows = df.shape[0]
#     train = int(.7 * rows)
#     validate = int(.1 * rows)
#     test = rows-train-validate

#     df.iloc[:train].to_csv(f"{base_dir}/train/train.csv",
#                             index=False,header=False)
#     df.iloc[train:(train+validate)].to_csv(f"{base_dir}/validation/validation.csv",
#                             index=False,header=False)
#     df.iloc[(train+validate):].to_csv(f"{base_dir}/test/test.csv",
#                             index=False,header=False)
