{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is used to configure RedShift ML\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)`\n",
    "\n",
    "This notebook trains a model directly from RedShift using SageMaker AutoPilot and the model is run in the RedShift cluster.\n",
    "\n",
    "Note: This notebook uses the model trained by RedShift ML for prediction and not the SageMaker endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of RedShift ML \n",
    "\n",
    "Amazon Redshift ML makes it easy for data analysts and database developers to create, train, and apply machine learning models using familiar SQL commands in Amazon Redshift data warehouses. Simply use SQL statements to create and train Amazon SageMaker machine learning models using your Redshift data and then use these models to make predictions. Redshift ML makes the model available as a SQL function within your Redshift data warehouse so you can easily apply it directly in your queries and reports. For example, you can use customer retention data in Redshift to train a churn detection model and then apply that model to your dashboards for your marketing team to offer incentives to customers at risk of churning.\n",
    "\n",
    "RedShift ML supports training a model using SageMaker AutoPilot or bring your own model (BYOM). BYOM supports both local and remote inference. Local mode runs the model created from a SageMaker training job or from a S3 path where the model artifacts are stored. Remote inference is using a SageMaker endpoint. For more information on BYOM, please refer to https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_MODEL.html#r_byom_create_model . When using BYOM, your inputs must match what the model is expecting. e.g. in notebook 04, you need to perform preprocessing of data first before sending the data to the model for predictions. Similarly, this has to be done when using BYOM.\n",
    "\n",
    "In this notebook, you will use RedShift ML which in turn uses SageMaker AutoPilot to train the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "Variable name for secret in Secret Manager, RedShift ML model and function name. RedShift, Athena and Glue information are stored in the secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_name='bankdm_redshift_login' \n",
    "\n",
    "model_name = 'dm01'\n",
    "function_name = 'predict_dm01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q SQLAlchemy==1.3.13\n",
    "!pip install psycopg2-binary pyathena\n",
    "!pip install -U pip\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from pyathena import connect\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create client session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get region \n",
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "# Get SageMaker session & default S3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "redshift = boto3.client('redshift')\n",
    "secretsmanager = boto3.client('secretsmanager')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get credentials & connection information from Secret Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    get_secret_value_response = secretsmanager.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    secret_arn=get_secret_value_response['ARN']\n",
    "\n",
    "except ClientError as e:\n",
    "    print(\"Error retrieving secret. Error: \" + e.response['Error']['Message'])\n",
    "    \n",
    "else:\n",
    "    # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "    if 'SecretString' in get_secret_value_response:\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    else:\n",
    "        secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            \n",
    "secret_json = json.loads(secret)\n",
    "master_user_name = secret_json['username']\n",
    "master_user_pw = secret_json['password']\n",
    "redshift_port = secret_json['port']\n",
    "redshift_cluster_identifier = secret_json['dbClusterIdentifier']\n",
    "redshift_endpoint_address = secret_json['host']\n",
    "\n",
    "database_name_redshift = secret_json['database_name_redshift']\n",
    "database_name_glue = secret_json['database_name_glue']\n",
    "\n",
    "schema_redshift = secret_json['schema_redshift']\n",
    "schema_athena = secret_json['schema_athena']\n",
    "\n",
    "table_name_glue = secret_json['table_name_glue']\n",
    "table_name_redshift = secret_json['table_name_redshift']\n",
    "\n",
    "# print(master_user_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RedShift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to RedShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = redshift.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "iam_role = response['Clusters'][0]['IamRoles'][0]['IamRoleArn']\n",
    "\n",
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(master_user_name, master_user_pw, redshift_endpoint_address, redshift_port, database_name_redshift))\n",
    "session = sessionmaker()\n",
    "session.configure(bind=engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model using SageMaker AutoPilot\n",
    "\n",
    "To train a model, use the `CREATE MODEL` SQL command in Redshift and specify training data either as a table or SELECT statement. Redshift ML then compiles and imports the trained model inside the Redshift data warehouse and prepares a SQL inference function that can be immediately used in SQL queries. Redshift ML automatically handles all the steps needed to train and deploy a model.\n",
    "\n",
    "After training with AutoPilot, the model will be run on the RedShift cluster itself and there are no additional charges on running the endpoint (AutoPilot charges still apply). \n",
    "\n",
    "In the code below, it creates a model using data from the entire table and the target column is `y`. The function name is also specified so it can be reference later on. \n",
    "\n",
    "For more information on the other parameters or on configuring RedShift ML (such as bringing your own model), please take a look at the documentation https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_MODEL.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = f\"\"\"\n",
    "CREATE MODEL {model_name}\n",
    "FROM {schema_redshift}.{table_name_redshift}\n",
    "TARGET y\n",
    "FUNCTION {function_name}\n",
    "IAM_ROLE '{iam_role}'\n",
    "SETTINGS (\n",
    "  S3_BUCKET '{bucket}'\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Other parameters you can set\n",
    "# [ MODEL_TYPE { XGBOOST | MLP } ]              \n",
    "# [ PROBLEM_TYPE ( REGRESSION | BINARY_CLASSIFICATION | MULTICLASS_CLASSIFICATION ) ]\n",
    "# [ OBJECTIVE ( 'MSE' | 'Accuracy' | 'F1' | 'F1Macro' | 'AUC') ]\n",
    "\n",
    "# print(statement)\n",
    "\n",
    "# The code below temporarily the isolation level in order to create the model using code. \n",
    "# When using the RedShift editor, changing the isolation level is not required.\n",
    "s = session()\n",
    "s.connection().connection.set_isolation_level(0)\n",
    "s.execute(statement)\n",
    "s.commit()\n",
    "s.connection().connection.set_isolation_level(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the status of the SageMaker AutoPilot training. \n",
    "Note: This takes approximately 92 minutes. \n",
    "\n",
    "While running, you can also look at the 'processing jobs' and 'training jobs' in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = f\"\"\"\n",
    "show model {model_name}\n",
    "\"\"\"\n",
    "\n",
    "# print(statement)\n",
    "df = pd.read_sql_query(statement, engine)\n",
    "# df.head(50)\n",
    "print(df.values[4][1])\n",
    "\n",
    "# This could take an hour\n",
    "while df.values[4][1] != 'READY':\n",
    "    time.sleep(10)\n",
    "    df = pd.read_sql_query(statement, engine)\n",
    "    print(df.values[4][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the details of the model\n",
    "Double check to ensure the `Model State` is `READY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = f\"\"\"\n",
    "show model {model_name}\n",
    "\"\"\"\n",
    "\n",
    "# print(statement)\n",
    "df = pd.read_sql_query(statement, engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the RedShift ML function with a SQL query.\n",
    "A SQL statement can be used to call the RedShift ML function that was created earlier. The SQL statement selects all the columns in the table except for the target column `y` and passes it to the RedShift ML function for prediction. Note that no preprocessing of the data is required here. The `y` column in the SQL statement is selected from the table to compare the predicted result (predict_dm01) vs the actual result (y). This is shown in the output below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = f\"\"\"\n",
    "SELECT {function_name}(\n",
    "                   age, job, marital, education, defaulted, housing,\n",
    "                   loan, contact, month, day_of_week, duration, campaign,\n",
    "                   pdays, previous, poutcome, emp_var_rate, cons_price_idx,\n",
    "                   cons_conf_idx, euribor3m, nr_employed), \n",
    "                   y\n",
    "          FROM {schema_redshift}.{table_name_redshift}\n",
    "\"\"\"\n",
    "\n",
    "# print(statement)\n",
    "df = pd.read_sql_query(statement, engine)\n",
    "# Check the accuracy for the first 10 predictions\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the overall accuracy\n",
    "The SQL statement below outputs a 'confusion matrix' like table where it shows how many predictions were correct or incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = f\"\"\"\n",
    "SELECT {function_name}, y, COUNT(*)\n",
    "  FROM (SELECT {function_name}(\n",
    "                   age, job, marital, education, defaulted, housing,\n",
    "                   loan, contact, month, day_of_week, duration, campaign,\n",
    "                   pdays, previous, poutcome, emp_var_rate, cons_price_idx,\n",
    "                   cons_conf_idx, euribor3m, nr_employed), y\n",
    "          FROM {schema_redshift}.{table_name_redshift})\n",
    " GROUP BY {function_name}, y;\n",
    "\"\"\"\n",
    "\n",
    "# print(statement)\n",
    "df = pd.read_sql_query(statement, engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next steps\n",
    "\n",
    "This is the end of the demo. Please proceed to notebook06 to clean up the resources created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
